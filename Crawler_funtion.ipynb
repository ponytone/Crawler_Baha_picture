{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    樓層    推   噓   差\n",
      "0   樓主  120  24  96\n",
      "3  4 樓   36   0  36\n",
      "樓主\n",
      "\t沒圖啦\n",
      "4 樓\n",
      "\t沒圖啦\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def baha_picture(url,page_number,gp_number):\n",
    "    # load\n",
    "    data = requests.get(url)\n",
    "    soup = BeautifulSoup(data.text,'lxml')\n",
    "    # Find page\n",
    "    di = soup.find('div',id=\"BH-master\")\n",
    "    s = di.find_all(href = re.compile(\"page=\"))\n",
    "    l = []\n",
    "    for i in s:\n",
    "        try:\n",
    "            l.append(int(i.contents[0]))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    page = max(l)\n",
    "    if page_number > page:\n",
    "        page_number = page\n",
    "    # Get all pages of url\n",
    "    pageList = []\n",
    "    if len(url.split('page')) == 1:\n",
    "        url = url.split(\"php?\")[0]+\"php?\"+\"page=1&\"+url.split(\"php?\")[1]\n",
    "    for i in range(1,page+1):\n",
    "        pageList.append(url.split(\"page=\")[0]+\"page=\"+str(i)+\"&bsn\"+url.split(\"&bsn\")[1])\n",
    "        \n",
    "    # Get gp / bp / floor / picture\n",
    "    gp = []\n",
    "    bp = []\n",
    "    a_list = []\n",
    "    pic_list = []\n",
    "    floor_list = []\n",
    "    n = 0\n",
    "    for page in pageList[:page_number]:\n",
    "        url = page\n",
    "        data = requests.get(url)\n",
    "        soup = BeautifulSoup(data.text,'lxml')\n",
    "        floor = soup.find_all('a',class_ =\"count tippy-gpbp-list\")\n",
    "        # gp\n",
    "        for i in floor[::2]:\n",
    "            if i.contents[0] == '爆':\n",
    "                gp.append(1000)\n",
    "            else:\n",
    "                gp.append(int(i.contents[0])) \n",
    "        # bp\n",
    "        for i in floor[1::2]:\n",
    "            if i.contents[0] == 'X':\n",
    "                bp.append(1000)\n",
    "            elif i.contents[0] == '-':\n",
    "                bp.append(0)\n",
    "            else:\n",
    "                bp.append(int(i.contents[0]))\n",
    "        #floor\n",
    "        alive = soup.find_all('a',class_ = \"floor\")\n",
    "        for i in alive:\n",
    "            a_list.append(i.contents[0])\n",
    "        # picture\n",
    "        picture = soup.find_all('div',class_ = \"c-section__main c-post \")\n",
    "        for i in range(len(picture)):\n",
    "            pic_list.append([])\n",
    "            try:\n",
    "                #pic_list[i].append(picture[i].find('a',class_=\"floor\").contents[0])\n",
    "                [pic_list[n+i].append(j['href']) for j in picture[i].find_all('a',href = re.compile(\"\\/truth.bahamut.com.tw\\/\"))]\n",
    "            except IndexError:\n",
    "                continue\n",
    "        n+=len(picture)\n",
    "    # Prepare for Dataframe\n",
    "    a_list = np.array(a_list).reshape(-1,1)\n",
    "    bp = np.array(bp).reshape(-1,1)\n",
    "    gp = np.array(gp).reshape(-1,1)\n",
    "    diff = gp - bp\n",
    "    floor_list = np.array(floor_list).reshape(-1,1)\n",
    "    pic_list = np.array(pic_list).reshape(-1,1)\n",
    "\n",
    "    # Turn into Dataframe\n",
    "    dataf = np.concatenate((bp,diff),axis = 1)\n",
    "    dataf = np.concatenate((gp,dataf),axis = 1)\n",
    "    dataf = np.concatenate((a_list,dataf),axis = 1)\n",
    "    dataf1 = np.concatenate((dataf,pic_list),axis = 1)\n",
    "    Liketable = pd.DataFrame(dataf1,columns = (\"樓層\",\"推\",\"噓\",\"差\",\"網址\"))\n",
    "    Liketable_simple = pd.DataFrame(dataf,columns = (\"樓層\",\"推\",\"噓\",\"差\"))\n",
    "    pd.set_option('max_colwidth', 200)\n",
    "\n",
    "    # Turn into int\n",
    "    Liketable[\"推\"] = pd.to_numeric(Liketable[\"推\"])\n",
    "    Liketable[\"噓\"] = pd.to_numeric(Liketable[\"噓\"])\n",
    "    Liketable[\"差\"] = pd.to_numeric(Liketable[\"差\"])\n",
    "    Liketable_simple[\"推\"] = pd.to_numeric(Liketable_simple[\"推\"])\n",
    "    Liketable_simple[\"噓\"] = pd.to_numeric(Liketable_simple[\"噓\"])\n",
    "    Liketable_simple[\"差\"] = pd.to_numeric(Liketable_simple[\"差\"])\n",
    "    \n",
    "    # Filter\n",
    "    filter1 = (Liketable_simple['差'] >= gp_number)\n",
    "    print(Liketable_simple[filter1])\n",
    "\n",
    "    filter1 = (Liketable['差'] >= gp_number)\n",
    "    for i in Liketable[filter1][\"樓層\"]:\n",
    "        fliter = (Liketable['樓層']== i)\n",
    "        print(i)\n",
    "        for x in Liketable[fliter][\"網址\"]:\n",
    "            if x == []:\n",
    "                print(\"\\t沒圖啦\")\n",
    "            else:\n",
    "                [print(z) for z in x]\n",
    "\n",
    "url = \"https://forum.gamer.com.tw/C.php?bsn=60076&snA=4051045&tnum=13669\"\n",
    "baha_picture(url,10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        樓層     推   噓    差\n",
      "0       樓主  1000  37  963\n",
      "7      8 樓   116   0  116\n",
      "10    11 樓    25   0   25\n",
      "12    13 樓    37   0   37\n",
      "24    26 樓    42   0   42\n",
      "133  144 樓    20   0   20\n",
      "149  161 樓    26   0   26\n",
      "154  166 樓    43   0   43\n",
      "158  170 樓    24   0   24\n",
      "175  187 樓    38   0   38\n",
      "樓主\n",
      "沒圖啦\n",
      "8 樓\n",
      "沒圖啦\n",
      "11 樓\n",
      "沒圖啦\n",
      "13 樓\n",
      "沒圖啦\n",
      "26 樓\n",
      "沒圖啦\n",
      "144 樓\n",
      "https://truth.bahamut.com.tw/s01/201512/4fab014d94d071e40e3bf417043e3509.PNG\n",
      "161 樓\n",
      "https://truth.bahamut.com.tw/s01/201512/0fa69a232b4cdbddca804f922d1e1f0b.JPG\n",
      "166 樓\n",
      "https://truth.bahamut.com.tw/s01/201512/0c9156e81b55b55142372106f4e1247d.JPG\n",
      "https://truth.bahamut.com.tw/s01/201512/211661be4f75b8da96b877e3483fa80a.JPG\n",
      "https://truth.bahamut.com.tw/s01/201512/3623c24fcecc5b103bdd72d2fbb3c8e5.JPG\n",
      "170 樓\n",
      "https://truth.bahamut.com.tw/s01/201512/008a0f5691b971ac95f05c3a22274ff4.PNG\n",
      "https://truth.bahamut.com.tw/s01/201704/7129c9861d1f35f9d7a81136e3215220.JPG\n",
      "187 樓\n",
      "https://truth.bahamut.com.tw/s01/201512/9ec21b52fa92e2d0c86250dcff2c614f.JPG\n",
      "https://truth.bahamut.com.tw/s01/201512/db1499ed8cf2b1a73cbef65dc4085657.JPG\n",
      "https://truth.bahamut.com.tw/s01/201512/367e5e1ca7cbdf1fcb505ca620f55d25.JPG\n",
      "https://truth.bahamut.com.tw/s01/201512/61f60a41c66355b2018befb5d9aad652.JPG\n",
      "https://truth.bahamut.com.tw/s01/201512/bc684030ac27ecdef8e4cc9fb18bf9a7.JPG\n",
      "https://truth.bahamut.com.tw/s01/201512/115932cf9f28ce8db62b154f41f02e87.JPG\n",
      "https://truth.bahamut.com.tw/s01/201512/4cfe47a5f167ba590cd645888cec5b39.JPG\n",
      "https://truth.bahamut.com.tw/s01/201512/45da0d5214e4bbcc8a602346f519a7cb.JPG\n",
      "https://truth.bahamut.com.tw/s01/201512/706ea54a4127adda72e34a654970ef91.JPG\n",
      "https://truth.bahamut.com.tw/s01/201512/1ccea7aa5ea83a9fa90d90259544b448.JPG\n"
     ]
    }
   ],
   "source": [
    "url = \"https://forum.gamer.com.tw/C.php?bsn=60076&snA=3101578\"\n",
    "page_number = 10\n",
    "gp_number = 15\n",
    "data = requests.get(url)\n",
    "soup = BeautifulSoup(data.text,'lxml')\n",
    "# Find page\n",
    "di = soup.find('div',id=\"BH-master\")\n",
    "s = di.find_all(href = re.compile(\"page=\"))\n",
    "l = []\n",
    "for i in s:\n",
    "    try:\n",
    "        l.append(int(i.contents[0]))\n",
    "    except ValueError:\n",
    "        continue\n",
    "page = max(l)\n",
    "if page_number > page:\n",
    "    page_number = page\n",
    "# Get all pages of url\n",
    "pageList = []\n",
    "if len(url.split('page')) == 1:\n",
    "    url = url.split(\"php?\")[0]+\"php?\"+\"page=1&\"+url.split(\"php?\")[1]\n",
    "for i in range(1,page+1):\n",
    "    pageList.append(url.split(\"page=\")[0]+\"page=\"+str(i)+\"&bsn\"+url.split(\"&bsn\")[1])\n",
    "    \n",
    "# Get gp / bp / floor / picture\n",
    "gp = []\n",
    "bp = []\n",
    "a_list = []\n",
    "pic_list = []\n",
    "floor_list = []\n",
    "n = 0\n",
    "for page in pageList[:page_number]:\n",
    "    url = page\n",
    "    data = requests.get(url)\n",
    "    soup = BeautifulSoup(data.text,'lxml')\n",
    "    floor = soup.find_all('a',class_ =\"count tippy-gpbp-list\")\n",
    "    # gp\n",
    "    for i in floor[::2]:\n",
    "        if i.contents[0] == '爆':\n",
    "            gp.append(1000)\n",
    "        else:\n",
    "            gp.append(int(i.contents[0])) \n",
    "    # bp\n",
    "    for i in floor[1::2]:\n",
    "        if i.contents[0] == 'X':\n",
    "            bp.append(1000)\n",
    "        elif i.contents[0] == '-':\n",
    "            bp.append(0)\n",
    "        else:\n",
    "            bp.append(int(i.contents[0]))\n",
    "    #floor\n",
    "    alive = soup.find_all('a',class_ = \"floor\")\n",
    "    for i in alive:\n",
    "        a_list.append(i.contents[0])\n",
    "    # picture\n",
    "    picture = soup.find_all('div',class_ = \"c-section__main c-post \")\n",
    "    for i in range(len(picture)):\n",
    "        pic_list.append([])\n",
    "        try:\n",
    "            #pic_list[i].append(picture[i].find('a',class_=\"floor\").contents[0])\n",
    "            [pic_list[n+i].append(j['href']) for j in picture[i].find_all('a',href = re.compile(\"\\/truth.bahamut.com.tw\\/\"))]\n",
    "        except IndexError:\n",
    "            continue\n",
    "    n+=len(picture)\n",
    "# Prepare for Dataframe\n",
    "a_list = np.array(a_list).reshape(-1,1)\n",
    "bp = np.array(bp).reshape(-1,1)\n",
    "gp = np.array(gp).reshape(-1,1)\n",
    "diff = gp - bp\n",
    "floor_list = np.array(floor_list).reshape(-1,1)\n",
    "pic_list = np.array(pic_list).reshape(-1,1)\n",
    "\n",
    "# Turn into Dataframe\n",
    "dataf = np.concatenate((bp,diff),axis = 1)\n",
    "dataf = np.concatenate((gp,dataf),axis = 1)\n",
    "dataf = np.concatenate((a_list,dataf),axis = 1)\n",
    "dataf1 = np.concatenate((dataf,pic_list),axis = 1)\n",
    "Liketable = pd.DataFrame(dataf1,columns = (\"樓層\",\"推\",\"噓\",\"差\",\"網址\"))\n",
    "Liketable_simple = pd.DataFrame(dataf,columns = (\"樓層\",\"推\",\"噓\",\"差\"))\n",
    "pd.set_option('max_colwidth', 200)\n",
    "\n",
    "# Turn into int\n",
    "Liketable[\"推\"] = pd.to_numeric(Liketable[\"推\"])\n",
    "Liketable[\"噓\"] = pd.to_numeric(Liketable[\"噓\"])\n",
    "Liketable[\"差\"] = pd.to_numeric(Liketable[\"差\"])\n",
    "Liketable_simple[\"推\"] = pd.to_numeric(Liketable_simple[\"推\"])\n",
    "Liketable_simple[\"噓\"] = pd.to_numeric(Liketable_simple[\"噓\"])\n",
    "Liketable_simple[\"差\"] = pd.to_numeric(Liketable_simple[\"差\"])\n",
    "\n",
    "# Filter\n",
    "filter1 = (Liketable_simple['差'] >= gp_number)\n",
    "print(Liketable_simple[filter1])\n",
    "\n",
    "filter1 = (Liketable['差'] >= gp_number)\n",
    "for i in Liketable[filter1][\"樓層\"]:\n",
    "    fliter = (Liketable['樓層']== i)\n",
    "    print(i)\n",
    "    for x in Liketable[fliter][\"網址\"]:\n",
    "        if x == []:\n",
    "            print(\"沒圖啦\")\n",
    "        else:\n",
    "            [print(z) for z in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
